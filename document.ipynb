{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Page Summarizer (Task 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**\n",
    "\n",
    "This project is a task from the HNG Machine learning Internship 2019. The model built in this notebook gives an abstractive summary of a page/article. Basically the model is trained to go through the page, get the major information from it and then give a summary of the page. Explanations of how the model works and how it is trained is presented in the notebook.\n",
    "\n",
    "**Libraries Used**\n",
    "\n",
    "We import the necessary libraries needed to manipulate the data, for numerical computation and for building our model. \n",
    "\n",
    "* RegEx - RegEx is imported as re. A RegEx, or Regular Expression, is a sequence of characters that forms a search pattern. RegEx can be used to check if a string contains the specified search pattern. It is imported as re.\n",
    "* Gensim - Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. It is a useful library for NLP.\n",
    "* Numpy - Numpy is used for scientific computations. It is imported as np.\n",
    "* Sklearn - Scikit-learn is a library in Python that provides many unsupervised and supervised learning algorithms. In this project we make use of the cosine similarity to measure similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import re\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading Data**\n",
    "\n",
    "Our data is contained in \"mayowa.txt\" file and we load it and save it as file object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading file\n",
    "file = open(\"mayowa.txt\",\"r\") \n",
    "data=file.readlines() \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of Data\n",
    "\n",
    "A function is defined below to process the text. The function first converts the text to lowerstring (Capital to small letter, then it checks if any of [\\(\\[].*?[\\)\\]] is in the new string and removes any of them found. The function then removes whitespaces and also removes numbers from the string. The processed strings which now consist of just lower cased words is now split into a list (tokens). The function then takes the list of words joins them with a whitespace and them removes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define preprocessing steps\n",
    "#lower case\n",
    "#remove everything inside []\n",
    "#remove 's\n",
    "#fetch only ascii characters\n",
    "\n",
    "def preprocessor(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", newString)\n",
    "    newString = re.sub(\"'s\",\"\",newString)\n",
    "    newString = re.sub(\"[^'0-9.a-zA-Z]\", \" \", newString)\n",
    "    tokens=newString.split()\n",
    "    return (\" \".join(tokens)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above is called below and for each text in the data is passed/processed through it and saved in the list (text). Each text saved in the list is then seperated with a (.). Each of the sentences is then checked for whitespaces and if any is found it is stripped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call above function\n",
    "text=[]\n",
    "for i in data:\n",
    "    text.append(preprocessor(i))\n",
    "\n",
    "all_sentences=[]    \n",
    "for i in text:\n",
    "    sentences=i.split(\".\")       \n",
    "    for i in sentences:\n",
    "        if(i!=''):\n",
    "            all_sentences.append(i.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences are then tokenized to be used for training in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the sentences for training word2vec\n",
    "tokenized_text = [] \n",
    "for i in all_sentences:\n",
    "    tokenized_text.append(i.split()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used for training is the word2vec model from the Gensim library and various parameters are speculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define word2vec model\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_text,\n",
    "            size=200, # desired no. of features/independent variables \n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 0, # 1 for cbow model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is then trained on the tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19002, 35240)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train word2vec\n",
    "model_w2v.train(tokenized_text, total_examples= len(tokenized_text), epochs=model_w2v.epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is defined below to obtain sentence embedding. It works by first creating a numpy array of zeros in the size variable specified and then check if word is present in vocabulary. If word is not present continue if count is not zero and vec is not equal to count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to obtain sentence embedding\n",
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#call above function\n",
    "wordvec_arrays = np.zeros((len(tokenized_text), 200))\n",
    "for i in range(len(tokenized_text)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_text[i], 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity between both word vectors arrays is then computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(wordvec_arrays), len(wordvec_arrays)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute similarity score\n",
    "for i in range(len(wordvec_arrays)):\n",
    "  for j in range(len(wordvec_arrays)):\n",
    "    if i != j:\n",
    "      sim_mat[i][j] = cosine_similarity(wordvec_arrays[i].reshape(1,200), wordvec_arrays[j].reshape(1,200))[0,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graph of the similarity is also generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a graph\n",
    "nx_graph = nx.from_numpy_array(sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute pagerank scores\n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the scores\n",
    "sorted_x = sorted(scores.items(), key=lambda kv: kv[1],reverse=True)\n",
    "\n",
    "sent_list=[]\n",
    "for i in sorted_x:\n",
    "    sent_list.append(i[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample Summary is presented below to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the highlights for him included hitting a hundred with a swollen jaw and helping india avoid the follow on in the process at georgetown in the drawn first test contributing with a fifty and four catches to india victory in the second test at port of spain india first test victory in west indies since 1975 76 and another fifty in the drawn fourth test with a wicket to boot that of ridley jacobs who was batting on 118. dravid who had been knocking at the doors of indian national cricket team for quite a while with his consistent performance in domestic cricket received his first national call in october 1994 for the last two matches of the wills world series. when dravid joined laxman in the middle on the third day of the test with scoreboard reading 232 4 and india still needing 42 runs to avoid an innings defeat another convincing win for australia looked inevitable. india failed to qualify for the semi finals having lost to australia and new zealand but achieved a consolation victory against pakistan in a tense game what with the military conflict going on between the two countries in kashmir at the same time. dravid arrived in south africa with the indian squad to participate in the 2003 cricket world cup in the capacity of first choice keeper batsman as part of their seven batsmen four bowlers strategy an experiment that had brought success to the team in the past year. the highlight for dravid was 75 runs scored in the tough fourth innings chase of the second test a crucial contribution to india first test win in sri lanka since 1993 despite the absence of key players like tendulkar laxman srinath and kumble. having regained his form on the tour to west indies where he scored a match winning hundred in sabina park jamaica dravid then toured england in what was billed as the series which would decide the world no. he played as designated keeper in six of the 7 match bilateral odi series and effected seven dismissals but fared poorly with the bat as india were handed a 2 5 drubbing by the new zealand. dravid scored 81 runs in the first innings of the third test and took 4 catches in the match as india defeated australia at chennai in a nail biting finish to clinch the series 2 1. dravid scored 85 runs in a match against zimbabwe in the 2000 01 coca cola champions trophy while opening the innings but was forced to miss the rest of the tournament because of an injury. \n"
     ]
    }
   ],
   "source": [
    "#extract top 10 sentences\n",
    "num=10\n",
    "summary=''\n",
    "for i in range(num):\n",
    "    summary=summary+all_sentences[sent_list[i]]+'. '\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8\n",
    "\n",
    "For task 8 a function is defined which that takes in the url of an article and returns the text in the url.\n",
    "\n",
    "### Libraries used\n",
    "\n",
    "Beautiful Soup library which is a very useful library for webscraping is used for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_content():  \n",
    "    \"\"\"\n",
    "    \n",
    "    :returns: text-> blah\n",
    "    :rtype: string\n",
    "    \n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "    response = requests.get(url)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html)\n",
    "    text = soup.text\n",
    "    return text\n",
    "\n",
    "\n",
    "print(read_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9\n",
    "\n",
    "Function that takes in title of article obtained from url, passes it through the page summarizer model and saves summary as a .txt file\n",
    "\n",
    "### Libraries Used\n",
    "\n",
    "**Beautiful Soup** - For scrapping data from web\n",
    "**NLTK** - Natural language toolkit helps build NLP models.\n",
    "**Pandas** - For manipulating data\n",
    "**Numpy** - For scientific computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Downloading stopwords and pukt package from Natural language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the stopwords package\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data used is obtained from Lucid blog post, 'On Time with Chibuike Osita'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping Data from Lucid blog On Time with Chibuike Osita\n",
    "\n",
    "url=\"https://lucid.blog/hngi6/post/qa-with-chibuike-osita-49e\"\n",
    "requested_url = urllib.request.urlopen(url).read().decode('utf8','ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using beautiful soup to read data from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the url\n",
    "\n",
    "soup= BeautifulSoup(requested_url, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* removing all text extracted from url with p tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all text that has p tag\n",
    "text_p = soup.find_all('p')\n",
    "print(text_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* making each text present lower cased and also tokenizing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(text_p)):\n",
    "    text += text_p[i].text\n",
    "text = text.lower()\n",
    "# tokenize the text\n",
    "tokens =[t for t in text.split()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removing irrelevant words, numbers or punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_token =tokens[:]\n",
    "#define irrelevant words that include stop words , punctuations and numbers\n",
    "stopword = set(stopwords.words('english') + list(punctuation))\n",
    "for token in tokens:\n",
    "    if token in stopword:\n",
    "        clean_token.remove(token)\n",
    "\n",
    "print(clean_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtaining the frequency distribution of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(clean_token)\n",
    "top_words=[]\n",
    "top_words=freq.most_common(100)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating a ranking for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating through all the sentences from the web to create a ranking for each sentence\n",
    "\n",
    "ranking = defaultdict(int)\n",
    "for i, sent in enumerate(sentences):\n",
    "    for word in word_tokenize(sent.lower()):\n",
    "        if word in freq:\n",
    "            ranking[i]+=freq[word]\n",
    "    top_sentences = nlargest(10, ranking, ranking.get)\n",
    "print(top_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing one of the top 2 sentences\n",
    "print(sentences[27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sorting through the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting all sentences\n",
    "sorted_sentences = [sentences[j] for j in sorted(top_sentences)]\n",
    "print(sorted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sorted_sentences.txt', 'w') as f:\n",
    "    for i in sorted_sentences:\n",
    "        f.write(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10\n",
    "\n",
    "API to handle text summarizer in task 9. It receives url as input and gives the summary of the article in the url as output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as p\n",
    "# import traceback\n",
    "from flask import Flask, request, jsonify\n",
    "import json\n",
    "app = Flask(__name__)\n",
    "\n",
    "#@app.before_request\n",
    "#def exe():\n",
    "#    summarizer = 'modelfile.pkl'\n",
    "#    model = p.load(open(summarizer, 'rb'))\n",
    "\n",
    "\n",
    "@app.route('/api/summarize', methods=['POST', 'GET'])\n",
    "def get_url():\n",
    "    \"\"\"\n",
    "    \n",
    "    :returns: jsonified-> blah\n",
    "    :rtype: string\n",
    "    \n",
    "    \"\"\"\n",
    "    if request.method == 'POST':\n",
    "        url = request.json['theUrl']\n",
    "        #print(content)\n",
    "        jsonified = jsonify(url), 200\n",
    "        return jsonified\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='127.0.0.1', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
